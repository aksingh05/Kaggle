{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n# Importing neccesary packages.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n#\n\nfrom scipy import interp\nimport math\nfrom scipy.stats import norm\nfrom scipy import stats\n\n#\n\nimport warnings\nwarnings.filterwarnings('ignore') # Disabling warnimgs for clearer outputs\npd.options.display.max_columns = 50 # Pandas option to increase max number of columns to display\nplt.style.use('ggplot') # Setting default plot style","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vtrain=pd.read_csv('/kaggle/input/titanic/train.csv')\nvtest=pd.read_csv('/kaggle/input/titanic/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#see data summary\nvtrain.info()\nvtrain.isnull().sum()\nvtest.info()\nvtest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging visualization datasets.\nvtrain.drop('PassengerId', axis=1, inplace=True)\nvtest.drop('PassengerId', axis=1, inplace=True)\nmerged = pd.concat([vtrain, vtest], sort=False).reset_index(drop=True)\n# Checking merged shape\ndisplay(merged.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corr = vtrain[['Survived', 'Age', 'Fare'\n                      ]].corr(method='spearman').abs().unstack().sort_values(\n                          kind='quicksort', ascending=False).reset_index()\n\n\ntrain_corr.rename(columns={\n    'level_0': 'Feature A',\n    'level_1': 'Feature B',\n    0: 'Correlation Coefficient'\n},\n                  inplace=True)\ntrain_corr\ntrain_corr[(train_corr['Feature A'] == 'Survived')].style.background_gradient(\n    cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#detecting missing values\nimport seaborn as sns\n%matplotlib inline\nfig, ax = plt.subplots(ncols=2, figsize=(20, 6))\nsns.heatmap(vtrain.isnull(),\n            yticklabels=False,\n            cbar=False,\n            cmap='magma',\n            ax=ax[0])\nsns.heatmap(vtest.isnull(),\n            yticklabels=False,\n            cbar=False,\n            cmap='magma',\n            ax=ax[1])\n\nax[0].set_title('Train Data Missing Values')\nax[1].set_title('Test Data Missing Values')\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def status(feature):\n    print('Processing', feature, ': DONE')\n    print(f'Shape after processing {combined.shape}')\n    print('*' * 40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_combined_data():\n    # Reading train data\n    train = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n    # Reading test data\n    test = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n    # extracting the targets from the training data\n    targets = train.Survived\n\n    # Merging train data and test data for future feature engineering\n    combined = train.append(test)\n    combined.reset_index(inplace=True, drop=True)\n\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_family():\n\n    global combined\n    # Introducing a new feature : The size of families (including the passenger)\n    combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1\n\n    # Introducing another feature based on the family size\n    combined['Alone'] = combined['FamilySize'].map(lambda s: 1\n                                                   if s == 1 else 0)\n\n    # These two below are optional, it didn't help with the model...\n    #combined['SmallFamily'] = combined['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n    #combined['LargeFamily'] = combined['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n\n    status('Family')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def family_survival():    \n    global combined\n\n    # A function working on family survival rate using last names and ticket features\n\n    # Extracting surnames\n    combined['Last_Name'] = combined['Name'].apply(\n        lambda x: str.split(x, \",\")[0])\n\n    # Adding new feature: 'Survived'\n    default_survival_rate = 0.5\n    combined['Family_Survival'] = default_survival_rate\n\n    for grp, grp_df in combined[[\n            'Survived', 'Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n            'SibSp', 'Parch', 'Age', 'Cabin'\n    ]].groupby(['Last_Name', 'Fare']):\n        \n        if (len(grp_df) != 1):\n            # A Family group is found.\n            for ind, row in grp_df.iterrows():\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    combined.loc[combined['PassengerId'] ==\n                                 passID, 'Family_Survival'] = 1\n                elif (smin == 0.0):\n                    combined.loc[combined['PassengerId'] ==\n                                 passID, 'Family_Survival'] = 0\n\n    for _, grp_df in combined.groupby('Ticket'):\n        if (len(grp_df) != 1):\n            for ind, row in grp_df.iterrows():\n                if (row['Family_Survival'] == 0) | (\n                        row['Family_Survival'] == 0.5):\n                    smax = grp_df.drop(ind)['Survived'].max()\n                    smin = grp_df.drop(ind)['Survived'].min()\n                    passID = row['PassengerId']\n                    if (smax == 1.0):\n                        combined.loc[combined['PassengerId'] ==\n                                     passID, 'Family_Survival'] = 1\n                    elif (smin == 0.0):\n                        combined.loc[combined['PassengerId'] ==\n                                     passID, 'Family_Survival'] = 0\n\n    status('FamilySurvival')\n    return combined\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_titles():\n\n    title_dictionary = {\n        'Capt': 'Dr/Clergy/Mil',\n        'Col': 'Dr/Clergy/Mil',\n        'Major': 'Dr/Clergy/Mil',\n        'Jonkheer': 'Honorific',\n        'Don': 'Honorific',\n        'Dona': 'Honorific',\n        'Sir': 'Honorific',\n        'Dr': 'Dr/Clergy/Mil',\n        'Rev': 'Dr/Clergy/Mil',\n        'the Countess': 'Honorific',\n        'Mme': 'Mrs',\n        'Mlle': 'Miss',\n        'Ms': 'Mrs',\n        'Mr': 'Mr',\n        'Mrs': 'Mrs',\n        'Miss': 'Miss',\n        'Master': 'Master',\n        'Lady': 'Honorific'\n    }\n\n    # Extract the title from names\n    combined['Title'] = combined['Name'].map(\n        lambda name: name.split(',')[1].split('.')[0].strip())\n\n    # Mapping titles\n    combined['Title'] = combined.Title.map(title_dictionary)\n    status('Title')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_names():\n    global combined\n    # Cleaning the Name variable\n    combined.drop('Name', axis=1, inplace=True)\n\n    # Encoding names as dummy variables\n    titles_dummies = pd.get_dummies(combined['Title'], prefix='Title')\n    combined = pd.concat([combined, titles_dummies], axis=1)\n\n    # Removing the title variable after getting dummies\n    combined.drop('Title', axis=1, inplace=True)\n\n    status('names')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_age():\n    global combined\n    # A function that fills the missing values of the Age variable\n    combined['Age'] = combined.groupby(\n        ['Pclass', 'Sex'])['Age'].apply(lambda x: x.fillna(x.median()))\n    status('Age')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def age_binner():    \n    global combined\n    # Ranging and grouping Ages\n    #bins = [0, 2, 18, 35, 65, np.inf]\n    names = ['less2', '2-18', '18-35', '35-65', '65plus']\n\n    #combined['AgeBin'] = pd.cut(combined['Age'], bins, labels=names)\n    combined['AgeBin'] = pd.qcut(combined['Age'],q = 5, labels = names)\n    age_dummies = pd.get_dummies(combined['AgeBin'], prefix='AgeBin')\n    combined = pd.concat([combined, age_dummies], axis=1)\n    combined.drop('AgeBin', inplace=True, axis=1)\n    combined.drop('Age', inplace=True, axis=1)\n    status('Age Bins')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_fares():\n    global combined\n\n    # Filling missing values in fare\n\n    combined['Fare'] = combined.groupby(\n        ['Pclass', 'Sex'])['Fare'].apply(lambda x: x.fillna(x.median()))\n    status('fare')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_fare_bin(onehot='None'):\n\n    global combined\n    # Ranging and grouping Fare using historical data\n    bins = [-1, 7.91, 14.454, 31, 99, 250, np.inf]\n    names = [0, 1, 2, 3, 4, 5]\n\n    combined['FareBin'] = pd.cut(combined['Fare'], bins,\n                                 labels=names).astype('int')\n    if onehot == 'yes':\n        farebin_dummies = pd.get_dummies(combined['FareBin'], prefix='FareBin')\n        combined = pd.concat([combined, farebin_dummies], axis=1)\n        combined.drop('FareBin', inplace=True, axis=1)\n        combined.drop('Fare', inplace=True, axis=1)\n    elif onehot == 'both':\n        farebin_dummies = pd.get_dummies(combined['FareBin'], prefix='FareBin')\n        combined = pd.concat([combined, farebin_dummies], axis=1)\n        combined.drop('FareBin', inplace=True, axis=1)\n    else:\n        combined.drop('Fare', inplace=True, axis=1)\n\n    status('FareBin')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_fare():\n\n    # A definition for scaling fare values\n\n    from scipy.stats import skew, boxcox_normmax, norm\n    from scipy.special import boxcox1p\n    global combined\n    combined['Fare'] = boxcox1p(combined['Fare'],\n                                boxcox_normmax(combined['Fare'] + 1))\n    status('NFareBin')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_embarked():\n    global combined\n    # Filling missing embarked values with the most frequent one\n    combined.Embarked.fillna(combined.Embarked.mode()[0], inplace=True)\n    # One hot encoding\n    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')\n    combined = pd.concat([combined, embarked_dummies], axis=1)\n    combined.drop('Embarked', axis=1, inplace=True)\n    status('Embarked')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_cabin():    \n    global combined\n    # Replacing missing cabins with M (for Missing)\n    combined['Cabin_Informed'] = [\n        1 if pd.notnull(cab) else 0 for cab in combined['Cabin']\n    ]\n    combined.Cabin.fillna('M', inplace=True)\n\n    # Mapping each Cabin value with the cabin letter\n    combined['Deck'] = combined['Cabin'].map(lambda c: c[0])\n\n    combined['Deck'].replace('T', 'A', inplace=True)\n\n    # One hot encoding ...\n    cabin_dummies = pd.get_dummies(combined['Deck'], prefix='Deck')\n    combined = pd.concat([combined, cabin_dummies], axis=1)\n\n    combined.drop('Cabin', axis=1, inplace=True)\n    combined.drop('Deck', axis=1, inplace=True)\n    status('Cabin')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_sex():\n    global combined\n    # Mapping string values with numerical ones\n    combined['Sex'] = combined['Sex'].map({'male': 1, 'female': 0})\n    status('Sex')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_pclass():\n    global combined\n    # One hot encoding pclass into 3 categories:\n    pclass_dummies = pd.get_dummies(combined['Pclass'], prefix='Pclass')\n\n    # Adding dummy variables to main set\n    combined = pd.concat([combined, pclass_dummies], axis=1)\n\n    # Removing redundant 'Pclass'\n    combined.drop('Pclass', axis=1, inplace=True)\n\n    status('Pclass')\n    return combined\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_ticket():\n\n    global combined\n\n    # A function that extracts each prefix of the ticket, returns 'Unknown' if no prefix (i.e the ticket is a digit)\n    def cleanTicket(ticket):\n        ticket = ticket.replace('.', '')\n        ticket = ticket.replace('/', '')\n        ticket = ticket.split()\n        ticket = map(lambda t: t.strip(), ticket)\n        ticket = list(filter(lambda t: not t.isdigit(), ticket))\n        if len(ticket) > 0:\n            return ticket[0]\n        else:\n            return 'Unknown'\n\n    # Extracting dummy variables from tickets:\n\n    combined['Ticket'] = combined['Ticket'].map(cleanTicket)\n    tickets_dummies = pd.get_dummies(combined['Ticket'], prefix='Ticket')\n    combined = pd.concat([combined, tickets_dummies], axis=1)\n    combined.drop('Ticket', inplace=True, axis=1)\n\n    status('Ticket')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Housekeeping\nThis function is for cleaning the redundant features after extracting useful information from them. The cabin, passengerId, last name, ticket and ofc our target survived is going to be dropped. These features did their job on feature engineering and we can leave them in peace now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def dropper():\n    global combined\n\n    combined.drop('Cabin', axis=1, inplace=True)\n    combined.drop('PassengerId', inplace=True, axis=1)\n    combined.drop('Last_Name', inplace=True, axis=1)\n    combined.drop('Survived', inplace=True, axis=1)\n    combined.drop('Ticket', inplace=True, axis=1)\n\n    return combined","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feeding the Machine\nOk this is our command room, this is where I control the switches for feature engineering(aka commenting some functions out or combine them :D) Well, this was pretty trial and error methodology for me. Tried couple combinations of feature engineering and tried to find best fit on the models. Here is what I came up with.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Executing definitions to process data\ncombined = get_combined_data()  # For merging train test data\ncombined = family_survival()  # For creating family survival feature\ncombined = process_family()  # For creating Family size feature\ncombined = get_titles()  # For extracting titles\ncombined = process_names()  # For one hot encoding titles\ncombined = process_age()  # For imputing missing age values\ncombined = age_binner()  # For grouping and hot encoding age ranges\ncombined = process_fares()  # For imputing fares\n# For grouping and label encoding fares, can use 'both' for keeping age with dummies or yes for just one hot\ncombined = process_fare_bin(onehot='no')\n# combined =scale_fare() # For scaling age values\ncombined = process_embarked()  # For imputing embarked and one hot encoding\n# combined = process_cabin() # For extracting deck info from cabins\ncombined = process_sex()  # For label encoding sex\n# combined = process_pclass() # For one hot encoding pclass\n# combined = process_ticket() # For extracting ticket info\ncombined = dropper()  # For dropping not needed features\nprint(\n    f'Processed everything. Missing values left: {combined.isna().sum().sum()}'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Double Check\nOk, this is the part where we're going make sure our data is ready for the modelling. So, let's start with checking how's our data looking. Everyting seems in order...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigning engineered data to inspection variable.\nv_merged = combined.copy()\nv_merged['Survived'] = vtrain['Survived']\nv_merged.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v_merged.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Matrix\nAlright one last table I'm going to check is correlation matrix. So we can spot linear relations between features. Especially the ones which effects survival rate. It seems Sex, the Mr. title and family survival ratio is most related features to survival.\n\nWell... That's it then let's continue with modelling now!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display numerical correlations between features on heatmap\nsns.set(font_scale=1.1)\ncorrelation_train = v_merged.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(18, 15))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\nSince preprocessing done we are ready for training our models. We start with loading packages and splitting our transformed data so we have 22 features and and 891 observations to train our estimators. Our test set has 418 observations to make predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, learning_curve, cross_validate, train_test_split, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, plot_roc_curve, auc\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom mlxtend.plotting import plot_decision_regions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recover_train_test_target():\n    global combined\n\n    y = pd.read_csv('/kaggle/input/titanic/train.csv', usecols=['Survived'])['Survived']\n    X = combined.iloc[:len(vtrain)]\n    X_test = combined.iloc[len(vtrain):]\n\n    return X, X_test, y\n\n\nX, X_test, y = recover_train_test_target()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\nprint(f'X_test shape: {X_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection\nI choose couple models to try, mostly ensemble models with boosting but we also have some linear models like Logistic Regression. I tuned some these models with Optuna but I'm not going to include these meta parts on this notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Models\n\ncv = StratifiedKFold(10, shuffle=True, random_state=42)\n\ndef model_check(X, y, estimators, cv):\n    model_table = pd.DataFrame()\n    x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n    row_index = 0\n    for est in estimators:\n\n        MLA_name = est.__class__.__name__\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n        #    model_table.loc[row_index, 'MLA Parameters'] = str(est.get_params())\n\n        est.fit(x_train, y_train)\n        \n        model_table.loc[row_index, 'Train Accuracy Mean'] = est.score(x_train, y_train)\n        model_table.loc[row_index, 'Test Accuracy Mean'] = est.score(x_test, y_test)\n\n        row_index += 1\n\n        model_table.sort_values(by=['Test Accuracy Mean'],\n                            ascending=False,\n                            inplace=True)\n\n    return model_table\n\nrf = RandomForestClassifier(criterion='gini',\n                            n_estimators=1750,\n                            max_depth=7,\n                            min_samples_split=6,\n                            min_samples_leaf=6,\n                            max_features='auto',\n                            oob_score=True,\n                            random_state=42,\n                            n_jobs=-1,\n                            verbose=0)\n\nlg = lgb.LGBMClassifier(max_bin=4,\n                        num_iterations=550,\n                        learning_rate=0.0114,\n                        max_depth=3,\n                        num_leaves=7,\n                        colsample_bytree=0.35,\n                        random_state=42,\n                        n_jobs=-1)\n\nxg = xgb.XGBClassifier(\n    n_estimators=2800,\n    min_child_weight=0.1,\n    learning_rate=0.002,\n    max_depth=2,\n    subsample=0.47,\n    colsample_bytree=0.35,\n    gamma=0.4,\n    reg_lambda=0.4,\n    random_state=42,\n    n_jobs=-1,\n)\n\nsv = SVC(probability=True)\n\nlogreg = LogisticRegression(n_jobs=-1, solver='newton-cg')\n\ngb = GradientBoostingClassifier(random_state=42)\n\ngnb = GaussianNB()\n\nmlp = MLPClassifier(random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [rf, lg, xg, gb, sv, logreg, gnb, mlp]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Model Results\nHmm... It seems our top classifiers are getting similar results and they are mostly ensemble models. Our basic Logistic Regression did pretty well too! Gradient Boosting Classifier seems effected by high variance, we might try to fix it by hyperparameter tuning but I'd say we already have good number of decent estimators so we can continue with them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_models = model_check(X, y, estimators, cv)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting model performances\n# model_barplot(raw_models, 32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_model_full_data = rf.fit(X, y)\nprint(accuracy_score(y, rand_model_full_data.predict(X)))\ny_pred = rand_model_full_data.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\nsubmission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = test_df['PassengerId']\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submission.csv', header=True, index=False)\nsubmission_df.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}